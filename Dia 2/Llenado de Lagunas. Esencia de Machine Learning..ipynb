{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://www.protipoftheday.com/sites/default/files/cv3_help_me2.gif)\n",
    "\n",
    "En el taller pasado vimos la esencia del modelo de aprendizaje en Machine Learning. Esta está compuesta en tres partes:\n",
    "\n",
    "## Hipotésis\n",
    "\n",
    "Esta es la predicción base del modelo. En el caso del perceptrón, le poniamos un peso o importancia a cada caracteristica de los datos. Como ejemplo, vamos a tomar una forma más simple del problema de aprobación de crédito. Suponemos que solo tenemos la cantidad de veces que el cliente ha estado tarde en pagar la cuota de su crédito y sus ingresos mensuales. Estos dos factores, cada uno tiene su propio peso o importancia. Entonces tendríamos:\n",
    "\n",
    "$$ Aprobacion = Signo((Peso \\ Ingresos \\cdot Ingresos) + (Peso \\ Veces \\ Tarde \\cdot Veces \\ Tarde)) $$\n",
    "\n",
    "Como hipotésis inicial, el sentido común nos dice que aquellos que tienen muchos ingresos mensuales y que ha cumplido con su fecha de pago de cuotas en su historial de crédito se le deberia aprobar el crédito. Estos tienen más chance de pagar lo que le prestemos. Sin embargo, los que tienen pocos ingresos o que sufren de pagar tarde, pueden llegar a durar mucho sin pagar, generando perdidas. Entonces lo podemos ver de esta forma:\n",
    "\n",
    "##### Mientras más alto sean los ingresos y más bajos sean la veces que ha pagado tarde, más probabilidades hay de que le aprobemos el credito.\n",
    "\n",
    "Como esto influye en los pesos? Veamolos de la siguiente manera. Si tenemos el peso de los ingresos y el peso de las veces que está tarde iguales, como en 1, los ingresos siempre terminarán dando la decisión final. ** Por qué?** Los ingresos usualmente estan a una escala en los miles como, 10,000, 50,000, 100,000 y las veces tarde en unidades como 1,3,7,10. **Están a escalas diferentes**.\n",
    "\n",
    "$$ Signo((-1)(50,000) + (1)(5))  \\ siempre \\ dara \\ negativo!$$\n",
    "\n",
    "Por esto, los pesos se deben acomodar según la escala del valor que estan pesando. Por ahí mismo, los pesos indican como afecta cada caracteristica a la respuesta final. Si las personas de ingresos altos son usualmente aprobados, entonces decimos que **Los ingresos afectan positivamente a la aprobación**. Por otro lado, si las personas que están varias veces tarde en pagar la cuota del  crédito son negadas del mismo, entonces decimos que **el número de veces  que una persona esté tarde con el pago afecta negativamente a la aprobación**. En terminos estadisticos, los pesos indican la **correlación** entre la aprobación del crédito y las caracteristicas del cliente. Un ejemplo de hipotesis podría ser esta:\n",
    "\n",
    "$$ Aprobación = Signo((0.001)(Ingresos) + (-1)(Veces \\ Tarde)) $$ \n",
    "\n",
    "Si los ingresos son altos, y son llevados a la escala del número de veces que está tarde, y hacemos una suma, la misma terminará positiva. Sin embargo, si ha estado demasiadas veces tarde, se le negará el crédito. Si ambos valores están en la misma escala, antes de calcular los pesos, ** aquel con el peso más grande tiene más influencia en el resultado final.** La razón por la cual estos pesos son sumados, es para convertir todas las caracteristicas, que pueden tener diferentes escalas y valores, en un solo valor. Este valor final es el que es usado para tomar la decisión final de la hipotesis, y es como una combinación de todas las caracteristicas en un solo valor. Las características con más peso tienen gran importancia en la salida final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error \n",
    "\n",
    "El error es lo que mide que tanto nuestra hipotésis se equivocó. En el caso del perceptron, restamos el valor de la hipotésis al valor real. ** Que indica esto?** Si el cliente se supone que fuera aprobado (1), pero predecimos que era negado (0), el error sería:\n",
    "\n",
    "$$ y_{real} - y_{hipotesis} = 1 - 0 = 1 $$\n",
    "\n",
    "Por otro lado, si el cliente debía ser negado y lo aprobamos, el error es :\n",
    "\n",
    "$$ y_{real} - y_{hipotesis} = 0 - 1 = -1 $$\n",
    "\n",
    "En el caso en que tanto el valor real como el de la hipotésis es igual, el error sería 0. Pero, ** en qué importa la dirección del error?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Optimización \n",
    "\n",
    "Al calcular el error de la hipotésis, tenemos que asesorar donde se equivocaron los pesos. Cuando predecimos que un cliente era negado cuando era aprobado, el valor del error era 1. **Qué  dice esto sobre los pesos?** Esto dice que los pesos son muy bajos, y por tanto, la hipotésis esta lejos del valor real. Si incrementamos los pesos, en manera positiva, cuando los sumemos **estarán más cerca de tener un valor total positivo, y la predicción final estaría más cerca de ser 1**. Consideremos el caso en que tenemos esta hipotesis, con los pesos siendo los coeficientes:\n",
    "\n",
    "$$Aprobación = Signo((0.0001)(Ingresos) + (-2)(Veces \\ Tarde))$$\n",
    "\n",
    "Tenemos una persona que llego 1 vez tarde, gana 10,000 pesos  y se le aprobó el credito. Si reemplazamos las variables tendríamos:\n",
    "\n",
    "$Signo((0.0001)(10000) + (-2)(1)) = 1$\n",
    "\n",
    "$Signo(1 - 2) = 1$\n",
    "\n",
    "$Signo(-1) = 1$\n",
    "\n",
    "$ 0 \\neq 1$\n",
    "\n",
    "Esta hipotesis se equivoca en este caso. Como actualizamos los pesos segun el error, según la actualización del perceptron tendriamos:\n",
    "\n",
    "Para el peso del numero de tardanzas\n",
    "\n",
    "$ w_2' = w_2 + (real - prediccion)x_2 \\\\\n",
    "  w_2' = -2 + (1 - 0)(1) \\\\\n",
    "  w_2' = -2 + (1)(1)\\\\\n",
    "  w_2' = -1$ \n",
    "\n",
    "Con eso, podemos notar que el peso **aumento** debido a que nuestra predicción fue menor que el valor real. Si nuestra predicción fuera **mayor** que el valor real, los pesos **disminuyen** para acercarse más al valor real. Pero que pasá con el peso del ingreso?\n",
    "\n",
    "$ w_1' = w_1 + (real - prediccion)x_1 \\\\\n",
    "  w_1' = 0.0001 + (1 - 0)(10000) \\\\\n",
    "  w_1' = 0.0001 + (1)(10000) \\\\\n",
    "  w_1' = 10000.0001$ \n",
    "\n",
    "El peso aumentó demasiado. Esto haría que los pesos relacionados con el ingreso cambien de valor bruscamente. Para eso introducimos un valor **alpha o tasa de aprendizaje**. Este valor regula que tan bruscamente se actualizan los valores de los pesos. \n",
    "\n",
    "Si alpha = 0.0001:\n",
    "\n",
    "$ w_1' = w_1 + alpha(real - prediccion)x_1 \\\\\n",
    "  w_1' = 0.0001 + 0.0001(1 - 0)(10000) \\\\\n",
    "  w_1' = 0.0001 + 0.0001(1)(10000) \\\\\n",
    "  w_1' = 0.0002$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el peso se actualiza de una manera menos brusca. Sin embargo, el peso de las tardanzas tambien se actualiza de manera más lenta. Por eso, es una buena práctica tener todas las características a la misma escala. También vale la pena saber que cuando la hipotesis de un ejemplo es igual que el valor real, el peso se mantiene igual:\n",
    "\n",
    "$ w' = w + alpha(real - prediccion)x \\\\\n",
    "  w' = w + alpha(0)x \\\\ \n",
    "  w' = w + 0\\\\\n",
    "  w' = w$\n",
    "  \n",
    "Cuando tenemos muchos datos, tendremos un cierto numero de errores inicialmente. Con el proceso de actualización de los pesos, estos errores  irán disminuyendo, hasta que lleguemos a un punto en que los errores se quedan entre dos valores que aumentan y luego disminuyen. Los mismo pasará con los pesos, los cuales fluctuan entre dos valores, no importa cuantas actualizaciones hagamos. A esto se le llama **convergencia**. Cuando llegamos a este punto, encontramos nuestra hipotésis ideal, y podemos parar el proceso de entrenamiento. Vamos a ver otro modelo de aprendizaje llamado ** Tocón de Decision o Decision Stump** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Stump\n",
    "\n",
    "El DS es un algoritmo de aprendizaje extremadamente simple, que solo utiliza una sola caracteristica de los datos para hacer una predicción. Al igual qué con el perceptrón vamos por todas las partes del modelo.\n",
    "\n",
    "#### Hipotésis\n",
    "\n",
    "La hipotésis del DS es la siguiente: \n",
    "\n",
    "##### Elegimos un valor limite. Si un valor es menor que el límite, predecimos falso (0). Si un valor es mayor o igual al limite, predecimos verdadero (1). \n",
    "\n",
    "De manera matematica:\n",
    "\n",
    "$$ h(x) = x \\geq limite $$\n",
    "\n",
    "Esto hace que el modelo sea algo binario, True si es mayor o igual, False de lo contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Stump_Hipotesis(valores,limite):\n",
    "    return valores >= limite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error \n",
    "\n",
    "El error en el DS es el total de ejemplos que fueron clasificados correctamente. Tambien conocido como la **certeza**. Pero como buscamos la menor cantidad de errores, calculamos el porcentaje de ejemplos clasificados incorrectamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Stump_Error(real,hipotesis):\n",
    "    return (real != hipotesis).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimización\n",
    "\n",
    "Aquí hay otra gran diferencia. El objetivo de la optimización del DS es la siguiente:\n",
    "\n",
    "##### ¿Cuál valor para el límite hace que tengamos la mínima cantidad de errores?\n",
    "\n",
    "Para lograr este objetivo, solamente tenemos que hacer algo muy forzoso. ** Probamos todos los valores posibles para el límite, contamos los errores para cada límite, y elegimos el límite con el menor total de errores. ** Esto implica probar todos los valores posíbles, hasta encontrar el limite más optimo. \n",
    "\n",
    "Podemos combinar los 3 pasos para crear el modelo del Decision Stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Decision_Stump(X,y):\n",
    "    limite = X[0]\n",
    "    mejor_limite = -1\n",
    "    error_minimo = len(X)\n",
    "    \n",
    "    for valor in X:\n",
    "        print(\"Probando el limite {}\".format(valor))\n",
    "        \n",
    "        hipotesis = Stump_Hipotesis(X,valor)\n",
    "        Error = Stump_Error(y,hipotesis)\n",
    "        \n",
    "        print(\"Errores del limite: {}\".format(Error))\n",
    "        \n",
    "        if Error < error_minimo:\n",
    "            error_minimo = Error\n",
    "            mejor_limite = valor\n",
    "    print(\"Mejor limite: {}\".format(mejor_limite))\n",
    "    print(\"Error minimo: {}\".format(error_minimo))\n",
    "    print(\"Hipotesis final: Ingresos >= {}\".format(mejor_limite))\n",
    "    return mejor_limite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, por último, vamos a probar el modelo con datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando el limite 10\n",
      "Errores del limite: 4\n",
      "Probando el limite 30\n",
      "Errores del limite: 3\n",
      "Probando el limite 20\n",
      "Errores del limite: 3\n",
      "Probando el limite 25\n",
      "Errores del limite: 2\n",
      "Probando el limite 70\n",
      "Errores del limite: 3\n",
      "Probando el limite 80\n",
      "Errores del limite: 4\n",
      "Probando el limite 90\n",
      "Errores del limite: 5\n",
      "Probando el limite 60\n",
      "Errores del limite: 4\n",
      "Probando el limite 100\n",
      "Errores del limite: 6\n",
      "Probando el limite 150\n",
      "Errores del limite: 7\n",
      "Mejor limite: 25\n",
      "Error minimo: 2\n",
      "Hipotesis final: Ingresos >= 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ingresos = np.array([10,30,20,25,70,80,90,60,100,150])\n",
    "aprobaciones = np.array([0,1,0,1,1,1,1,0,1,0])\n",
    "\n",
    "Decision_Stump(ingresos,aprobaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y listo, un modelo que predice  la aprobación del crédito según ingresos. Para este modelo, las aplicaciones de clientes que tengan ingresos mensuales mayores que 25 mil pesos serán aprobadas, mientras que los que sean menor que 25 mil pesos serán negadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
